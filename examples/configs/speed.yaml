seed: 1337

device: "cuda"     # "cuda" or "cpu"
precision: "fp32"  # "fp16" | "bf16" | "fp32"

# TensorFloat-32 control for FP32 matmuls on Ampere+.
# PyTorch recommends controlling via torch.set_float32_matmul_precision. (high/medium use TF32)
tf32_matmul_precision: "high"  # "highest" | "high" | "medium"

cudnn_benchmark: true
deterministic: false

channels_last: false

amp:
  enabled: true
  grad_scaler: false    # mainly relevant for fp16; bf16 usually doesn't need scaling, but harmless.
  autocast_cache_enabled: true

optimizer:
  name: "adamw"
  lr: 0.0003
  weight_decay: 0.01
  fused: false
  foreach: null         # null = let PyTorch decide; true/false to force

compile:
  enabled: false # Windows does not support torch.compile in PyTorch 2.1
  backend: "inductor"
  mode: "max-autotune"  # "default" | "reduce-overhead" | "max-autotune" | "max-autotune-no-cudagraphs"
  fullgraph: false
  dynamic: false
  options:
    shape_padding: true  # aligns shapes for better Tensor Core usage (Inductor option)

sdp:
  enable_flash: true
  enable_mem_efficient: true
  enable_math: true
  enable_cudnn: true

train:
  epochs: 1
  steps_per_epoch: 200
  batch_size: 32
  grad_accum_steps: 1
  clip_grad_norm: 1.0
  log_every: 20
